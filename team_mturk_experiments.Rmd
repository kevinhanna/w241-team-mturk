---
title: "Team mTurk - Motivating Quality Work"
author: "Kevin Hanna, Kevin Stone, Changjing Zhao"
output: 
    github_document: 
      toc: true
      df_print: kable
    pdf_document: 
      toc: true
      df_print: kable
      highlight: tango
    html_document:
      toc: true
      df_print: kable
      highlight: tango

#knit: (function(inputFile, encoding) {
#  rmarkdown::render(
#    inputFile, encoding = encoding,
#    output_format ='github_document')
#    })
  
---

## Motivating Quality Work
### What motivates crowdsourced workers to do quality work?

Our scoring metric measures the accuracy of the bounding box by calculating the euclidean distance of the Turkers bounds to the correct bounding.
Therefor a **lower score is better**.  When the treatment should cause a negative reaction, the score should increase if our hypothesis is correct.


```{r setup, include=FALSE}
library(data.table)
library(dplyr)
library(rjson)
library(pwr)
library(stargazer)
library(knitr)
```

```{r load_data, echo=FALSE}
# Read in all the data, we'll use the 'experiment_no' to pull the results from each experiment. 
d <- fread("./data/experiment_all/experiment_results.csv")

# None of our experiments required creating multiple bounding boxes on a single image.
# We are assuming the extra bounding boxes are mistakes and taking the better score.
remove_extra_bounding_boxes <- function(d) {
  good_scores <- d[,  .(bounding_box_score = min(bounding_box_score), count = .N), keyby=.(WorkerId, ImageId, experiment_no, in_treatment)] %>% .[count<2, ]
  d[good_scores, on=.(WorkerId, ImageId, experiment_no, bounding_box_score, in_treatment)]
}

d <- remove_extra_bounding_boxes(d)

## Create a logical score (so that higher score is better)
max_bounding_box_score <- ceiling(max(d[, bounding_box_score], na.rm=T))
d[, regularized_score := (max_bounding_box_score - bounding_box_score)]

```

### Our Experiments


1. Pilot - Bound 20 images with negative treatment (Government Surveillance)
2. Pilot - Bound a single image with negative treatment (Government Surveillance)
3. Experiment - Bound a single image with positive treatment (Potential future work)
4. Experiment - Increase subjects for experiment 3
5. Experiment - Bound a single image with negative treatment, reward 2 cents (Threat of not paying for poor performance)
6. Experiment - Bound a single image with negative treatment, increased reward to 5 cents (Threat of not paying for poor performance)

```{r data_summary, echo=FALSE}
kable(
  d[, .(count=.N, mean_score=mean(bounding_box_score, na.rm=T), std_dev=sd(bounding_box_score, na.rm=T)), keyby=.(experiment_no, is_pilot, in_treatment)],
  caption = "Experiment Data Summary"
)

kable(summary(d[, .(bounding_box_score)]))

```


### Experiment 1, our first pilot

For our pilot, we gave the Turkers a negative treatment and asked that they draw a single bounding box on each of 20 images.  We first collected some information about the subject through a survey and then randomly assigned those subjects to treatment and control.  Our primary goal was to understand how our scoring scheme worked, gauge level of variance we should expect in future experiments and test if our covariates collected from our survey were helpful.  We had high attrition and due to a misunderstanding of the Mechanical Turk platform, our assignments to treatment and control failed and we ended up with Turkers not in our experiment in our results, and many ended up in both treatment and control.

We were not able to trust any ATE, but we could at least see the variance, which was exceptionally high.


```{r plot_experiment_1, echo=FALSE}
# Block the results by user taking their mean score

worker_mean_score <- d[experiment_no==1, .(mean_worker_score = mean(bounding_box_score), in_treatment = as.integer(median(in_treatment))), keyby=WorkerId]

dev_null <- worker_mean_score[, plot(mean_worker_score, col=(in_treatment+1))]
dev_null <- worker_mean_score[, plot(log(mean_worker_score), col=(in_treatment+1))]

```


```{r experiment_1_summary, echo=FALSE}

kable(summary(worker_mean_score[, .(mean_worker_score)]))
kable(worker_mean_score[, .(mean_score=mean(mean_worker_score, na.rm=T), std_dev=sd(mean_worker_score, na.rm=T)), keyby=.(in_treatment)])

```


```{r}
#TODO Gauge if effort decreases with more HITTs
```


### Experiment 2, our second pilot

With the first pilot behind us, we decided we needed to focus on increasing our statistical power and hypothesized that having more subjects with fewer experiments would provide more statistical power. 



```{r plot_experiment_2, echo=FALSE}
dev_null <- d[experiment_no==2, plot(bounding_box_score, col=(in_treatment+1))]
dev_null <- d[experiment_no==2, plot(log(bounding_box_score), col=(in_treatment+1))]
```


```{r experiment_2_summary, echo=FALSE}
summary(d[experiment_no==2, .(bounding_box_score)])
```

```{r experiment_2_summary_groups, echo=FALSE}

kable(d[experiment_no==2, .(mean_score=mean(bounding_box_score, na.rm=T), std_dev=sd(bounding_box_score, na.rm=T)), keyby=.(in_treatment)])
```





Even with a p-value of 0.655, this was progress.  Our coefficient for in treatment was still more likely due to random noise than not.  

#### 2.1 Power Test



```{r experiment_2_power_test, echo=FALSE}

e2_ate = d[experiment_no==2 & in_treatment == 1, mean(bounding_box_score, na.rm=T)] - d[experiment_no==2 & in_treatment == 0, mean(bounding_box_score, na.rm=T)]
e2_sd = d[experiment_no==2, sd(bounding_box_score, na.rm=T)]


power.t.test(delta=e2_ate, 
             sd=e2_sd, 
             sig.level = 0.05,
             power = 0.80,
             alternative = "one.sided",
             n = NULL)


```

To achieve the statistical power of 0.8 at the 0.05 confidence-level with the variance we had in this experiement, we would require nearly 5,800 subjects in both control and treatment.  


#### 2.2 Analysis

With this pilot, the only covariate we had was the amount of time each Turker spent on the task. And working time acts as a control explaining away some of the variance reducing the p-value for our target feature from 0.45 to 0.39.  

```{r experiment_2_regression, results='asis'}
e2_mod_1 <- d[experiment_no==2, lm(bounding_box_score ~ in_treatment)]
e2_mod_2 <- d[experiment_no==2, lm(bounding_box_score ~ in_treatment+WorkTimeInSeconds)]

stargazer(e2_mod_1,  e2_mod_2,
          type = 'text', header = FALSE, table.placement = 'h', report=('vc*p'),
          add.lines = list(c("Data Subset", "All", "All", "$x==1$")),
          column.labels=c("Target Alone", "With WorkInSeconds Control"))
```


```{r experiment_2_regression_time, echo=FALSE, results='asis'}
e2_mod_3 <- d[experiment_no==2, lm(WorkTimeInSeconds ~ in_treatment)]

stargazer(e2_mod_3,  
          type = 'text', header = FALSE, table.placement = 'h', report=('vc*p'),
          add.lines = list(c("Data Subset", "All", "All", "$x==1$")))
```

The results suggest the negative treatment caused Turkers to spend less time on the task, but the p-value is far from statistically significant again.

#### 2.3 Learnings from our second experiment.

The estimated 11,600 subjects required to achieve the statistical power we needed was far too many, 
With a p-value of 0.389, even with the 11,600 subjects, we weren't likely to find a statistically significant ATE.  We need to change our experiment and collect more covariates. 



### Experiment 3, incentive of future work.  

In both of our pilots, we used a treatment which we hypothesized would cause the Turkers in treatment to work less hard, and the ATE was positive, which in our scoring means the bounding was less accurate.  We also wanted to test if a positive treatment would have a larger ATE, so the Turkers in treatment were told we were looking for Turkers to perform some future work with the hypothesis that if the Turkers though of the task as a test with the incentive of future work they would try harder.  So we ran a small experiment to test this theory.

#### 3.1 Simple regression analysis

```{r}
e3_mod_1 <- d[experiment_no==3, lm(bounding_box_score ~ in_treatment)]

stargazer(e3_mod_1, e2_mod_1, 
          type = 'text', header = FALSE, table.placement = 'h', report=('vc*p'),
          add.lines = list(c("Data Subset", "All", "All", "$x==1$")),
          column.labels=c("Incentivized", "Negative Treatment"))
```

The first look was disappointing, the last p-value had gone up from 0.45 in the previous experiment to 0.563 in this, but we only used a quarter the number of subjects.  More concerning is the change in the treatment was estimated to change the direction of the coeffecient, and it is still positive.  

#### 3.2 Analysis with covariates

In this experiment we asked the Turkers to answer some questions about the device they were using, their experience doing these types of tasks and some demographic info. 

```{r}
e3_mod_2 <- d[experiment_no==3, lm(bounding_box_score ~ in_treatment+as.factor(monitor))]
e3_mod_3 <- d[experiment_no==3, lm(bounding_box_score ~ in_treatment+as.factor(didbf))]
e3_mod_4 <- d[experiment_no==3, lm(bounding_box_score ~ in_treatment+as.factor(age))]
e3_mod_5 <- d[experiment_no==3, lm(bounding_box_score ~ in_treatment+as.factor(edu))]
e3_mod_6 <- d[experiment_no==3, lm(bounding_box_score ~ in_treatment+as.factor(income))]

stargazer(e3_mod_1, e3_mod_2, e3_mod_3, e3_mod_4, e3_mod_5, e3_mod_6, 
          type = 'text', header = FALSE, table.placement = 'h', report=('vc*p'),
          add.lines = list(c("Data Subset", "All", "All", "$x==1$")),
          column.labels=c("Target Alone", "Monitor size", "Did task before", "Age", "Education", "Income"))
```

The only covariate which seemed to act as any type of control was the education question, though it wasn't very significant.  However, all of the coeffecients for the screensize question were negative, and by a fairly significant ammount.  The baseline value was cellphone, which can be significantly smaller than all the other types of screens.  So we tested that on its own.

```{r experiment_3_regression_cell, echo=FALSE, results='asis'}
e3_mod_7 <- d[experiment_no==3, lm(bounding_box_score ~ in_treatment+(monitor=="cellphone"))]

stargazer(e3_mod_1, e3_mod_7, 
          type = 'text', header = FALSE, table.placement = 'h', report=('vc*p'),
          add.lines = list(c("Data Subset", "All", "All", "$x==1$")),
          column.labels=c("Target Alone", "Used Cellphone"))
```

If the subject is using a cellphone to do the task, their accuracy goes down (score increases), which is intuitive.  Having cellphone as a control decreases the p-value from 0.56 to 0.33.  With more data, this could be quite a bit lower. But it still doesn't explain why subjects with more incentive are doing a poorer job.


As with the previous experiment, we also analyzed how the treatment affected the amount of time they spent on the task.  


```{r experiment_3_regression_time, echo=FALSE, results='asis'}
e3_mod_3 <- d[experiment_no==3, lm(WorkTimeInSeconds ~ in_treatment)]

stargazer(e3_mod_3, e2_mod_3,
          type = 'text', header = FALSE, table.placement = 'h', report=('vc*p'),
          add.lines = list(c("Data Subset", "All", "All", "$x==1$")),
          column.labels=c("Incentivized", "Negative Treatment"))
```

The regression shows those in treatment on average spent 23 seconds more time, this alone is concerning, as the task itself shouldn't take that much time.  


```{r experiment_3_summary_time, echo=FALSE, results='asis'}

hist(d[experiment_no==3, WorkTimeInSeconds])

```

There are alot of values suggesting that Turkers are not conentrating on our task, it could be they are spawning multiple tabs.  Regardless, working time is not helpful for our experiment. 

#### 3.1 Power Test

With a lot of speculation about whether our statistical significance would go up with more data, we tested that theory by doing a power calculation. 

```{r power_test_3, echo=FALSE}

e3_ate = d[experiment_no==3 & in_treatment == 1, mean(bounding_box_score, na.rm=T)] - d[experiment_no==3 & in_treatment == 0, mean(bounding_box_score, na.rm=T)]
e3_sd = d[experiment_no==3, sd(bounding_box_score, na.rm=T)]

power.t.test(delta=e3_ate, 
             sd=e3_sd, 
             sig.level = 0.05,
             power = 0.80,
             alternative = "one.sided",
             n = NULL)


```

The power calculation when using the negative treatment, telling those in treatment that they were doing work for a government surveillance system estimated we needed 5,800 subjects.  Using an incentive of possible future work as the treatment, the ATE has less variance, and estimated that we only need 835 subjects to get 0.80 statistical power. 


### Experiment 4, More data

To improve the statistical power from Experiment 3, we are adding more data and sending out another 100 control tasks to Turkers and 100 with the same treatment. 

#TODO covariate balance check, demographic info show how random it is.

```{r experiment_4_regression, echo=FALSE, results='asis'}
e4_mod_1 <- d[experiment_no %in% c(3,4), lm(bounding_box_score ~ in_treatment)]

stargazer(e4_mod_1, e3_mod_1,  
          type = 'text', header = FALSE, table.placement = 'h', report=('vc*p'),
          add.lines = list(c("Data Subset", "All", "All", "$x==1$")),
          column.labels=c("n=100 + n=200", "n=100"))
```

The results are much better, adding another 200 subjects helped decrease the p-value from 0.56 to 0.12, and our ATE is -15.9, a negative number means the bounding boxes from treatment are more accurate.  

```{r experiment_4_regression_worktime, echo=FALSE, results='asis'}
# Don't need this anymore
#e4_mod_4 <- d[experiment_no %in% c(3,4), lm(WorkTimeInSeconds ~ in_treatment)]

#stargazer(e4_mod_4, e3_mod_3, e2_mod_3,
#          type = 'text', header = FALSE, table.placement = 'h', report=('vc*p'),
#          add.lines = list(c("Data Subset", "All", "All", "$x==1$")))
```

```{r}
#e4_mod_2 <- d[experiment_no %in% c(3,4), lm(bounding_box_score ~ in_treatment+is_mobile+tried=(monitor=="" &	mousetrackpad==""))]
#summary(e4_mod_2)

#e4_mod_2 <- d[experiment_no %in% c(3,4), .(tried=as.numeric(is.na(monitor) & is.na(mousetrackpad)), bounding_box_score, in_treatment, is_mobile, WorkTimeInSeconds)] %>% 
#  .[, lm(bounding_box_score ~ in_treatment+is_mobile+tried)]
#summary(e4_mod_2)

#e4_mod_2 <- d[experiment_no %in% c(3,4), .(tried=as.numeric(is.na(monitor)), bounding_box_score, in_treatment, is_mobile, Reward)] %>% 
#  .[, lm(bounding_box_score ~ in_treatment+is_mobile+tried)]
#summary(e4_mod_2)

e4_mod_2 <- d[experiment_no %in% c(3,4), lm(bounding_box_score ~ in_treatment+is_mobile)]
stargazer(e4_mod_1, e4_mod_2,  
          type = 'text', header = FALSE, table.placement = 'h', report=('vc*p'),
          add.lines = list(c("Data Subset", "All", "All", "$x==1$")),
          column.labels=c("Target Alone", "Cellphone"))


```




```{r, results='asis'}
#e4_mod_3 <- d[experiment_no %in% c(3,4), lm(bounding_box_score ~ in_treatment+factor(monitor))]
#e4_mod_4 <- d[experiment_no %in% c(3,4), lm(bounding_box_score ~ in_treatment+factor(didbf))]
#e4_mod_5 <- d[experiment_no %in% c(3,4), lm(bounding_box_score ~ in_treatment+factor(age))]
#e4_mod_6 <- d[experiment_no %in% c(3,4), lm(bounding_box_score ~ in_treatment+factor(edu))]
#e4_mod_7 <- d[experiment_no %in% c(3,4), lm(bounding_box_score ~ in_treatment+factor(income))]

#stargazer(e4_mod_3, e4_mod_4, e4_mod_5, e4_mod_6, e4_mod_7,
#          type = 'text', header = FALSE, table.placement = 'h',  report=('vc*p'),
#          add.lines = list(c("Data Subset", "All", "All", "$x==1$")))

```




#### 4.1 Power Test
```{r}

e4_ate = d[experiment_no %in% c(3, 4) & in_treatment == 1, mean(bounding_box_score, na.rm=T)] - d[experiment_no %in% c(3, 4) & in_treatment == 0, mean(bounding_box_score, na.rm=T)]

e4_sd = d[experiment_no %in% c(3, 4), sd(bounding_box_score, na.rm=T)]


power.t.test(delta=abs(e4_ate), 
             sd=e4_sd, 
             sig.level = 0.05,
             power = 0.80,
             alternative = "one.sided",
             n = NULL)
```

```{r}

power_curve <- function(x) {
  result = c()

  for (i in 1:length(x)) {
    new_n <- power.t.test(delta=abs(e4_ate), 
             sd=e4_sd, 
             sig.level = 0.05,
             power = NULL,
             alternative = "one.sided",
             n = x[i])["power"]
    
    result <- c(result, new_n)
  }
  
  return(result)
}

sig_curve <- function(x) {
  result = c()

  for (i in 1:length(x)) {
    new_n <- power.t.test(delta=abs(e4_ate), 
             sd=e4_sd, 
             sig.level = NULL,
             power = 0.8,
             alternative = "one.sided",
             n = x[i])["sig.level"]
    
    result <- c(result, new_n)
  }
  
  return(result)
}

delta_curve <- function(x) {
  result = c()

  for (i in 1:length(x)) {
    new_n <- power.t.test(delta=x[i], 
             sd=e4_sd, 
             sig.level = 0.05,
             power = 0.8,
             alternative = "one.sided",
             n = NULL)["n"]
    
    result <- c(result, new_n)
  }
  
  return(result)
}
curve(power_curve(x), 10, 1000)
#curve(sig_curve(x), 10, 1000)
#curve(delta_curve(x), 5, 20)

```


### Experiment 5, threats don't work

```{r}

e5_mod_1 <- d[experiment_no == 5, lm(bounding_box_score ~ in_treatment)]
summary(e5_mod_1)
```


### Experiment 6, threats still don't work

```{r}

e6_mod_1 <- d[experiment_no == 6, lm(bounding_box_score ~ in_treatment)]
summary(e6_mod_1)
```

```{r}
e6_mod_2 <- d[experiment_no %in% c(5,6), lm(bounding_box_score ~ in_treatment+(Reward == "$0.05"))]
summary(e6_mod_2)
```


### Experiment 7, Even MORE incentive data

```{r experiment_4_regression, echo=FALSE, results='asis'}
e7_mod_1 <- d[experiment_no %in% c(3,4,7), lm(bounding_box_score ~ in_treatment)]
e7_mod_2 <- d[experiment_no %in% c(3,4,7), lm(bounding_box_score ~ in_treatment+is_cellphone)]

stargazer(e4_mod_1, e4_mod_2, e7_mod_1, e7_mod_2, 
          type = 'text', header = FALSE, table.placement = 'h', report=('vc*p'),
          add.lines = list(c("Data Subset", "All", "All", "$x==1$")),
          column.labels=c("n=300", "n=300 and cellphone","n=700", "n=700 and cellphone"))

```


```{r}

e7_ate = d[experiment_no %in% c(3, 4, 7) & in_treatment == 1, mean(bounding_box_score, na.rm=T)] - d[experiment_no %in% c(3, 4, 7) & in_treatment == 0, mean(bounding_box_score, na.rm=T)]

e7_sd = d[experiment_no %in% c(3, 4, 7), sd(bounding_box_score, na.rm=T)]


power.t.test(delta=abs(e7_ate), 
             sd=e7_sd, 
             sig.level = 0.05,
             power = 0.80,
             alternative = "one.sided",
             n = NULL)
```
