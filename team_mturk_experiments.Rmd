---
title: "Team mTurk - Motivating Quality Work"
author: "Kevin Hanna, Kevin Stone, Changjing Zhao"
output:
  pdf_document: 
    fig_caption: yes
    toc: yes
    toc_depth: 4
---

# Motivating Quality Work
## What motivates crowdsourced workers to do quality work?

Our scoring metric measures the accuracy of the bounding box by calculating the euclidean distance of the Turkers bounds to the correct bounding.
Therefor a **lower score is better**.  When the treatment should cause a negative reaction, the score should increase if our hypothesis is correct.


```{r setup, include=FALSE}
library(data.table)
library(dplyr)
#library(rjson)
library(pwr)
library(stargazer)
library(knitr)
#library(kableExtra)

```

```{r load_data, echo=FALSE}
# Read in all the data, we'll use the 'dataset_no' to pull the results from each experiment. 
d <- fread("./data/experiment_all/experiment_results.csv")

# None of our experiments required creating multiple bounding boxes on a single image.
# We are assuming the extra bounding boxes are mistakes and taking the better score.
remove_extra_bounding_boxes <- function(d) {
  good_scores <- d[,  .(bounding_box_score = min(bounding_box_score), count = .N), keyby=.(WorkerId, ImageId, experiment_no, in_treatment)] %>% .[count<2, ]
  d[good_scores, on=.(WorkerId, ImageId, experiment_no, bounding_box_score, in_treatment)]
}

d <- remove_extra_bounding_boxes(d)

## Create a logical score (so that higher score is better)
max_bounding_box_score <- ceiling(max(d[, bounding_box_score], na.rm=T))
d[, regularized_score := (max_bounding_box_score - bounding_box_score)]


## experiment_no doesn't make sense
d[, dataset_no := experiment_no]

d[dataset_no %in% c(1), experiment := "pilot"]
d[dataset_no %in% c(2), experiment := "social"]
d[dataset_no %in% c(3,4,7), experiment := "future"]
d[dataset_no %in% c(5,6), experiment := "immediate"]

# Add a 95th percentil dummy variable
d[, pct_95 := as.numeric(bounding_box_score<quantile(bounding_box_score, prob=c(0.05), na.rm=T)), by=experiment]
d[, pct_5 := as.numeric(bounding_box_score<quantile(bounding_box_score, prob=c(0.95), na.rm=T)), by=experiment]


```


```{r formatting_helper_functions, echo=FALSE}

# Formatting helpers

# PDF
stargazer_type="latex"

# Markdown
#stargazer_type="text"

format_table <- function(table_d, caption = NULL) {
  kable(table_d)
}

```


## Our Datasets


1. Bound 20 images with negative treatment (Government Surveillance)
2. Bound a single image with negative treatment (Government Surveillance)
3. Bound a single image with positive treatment (Potential future work)
4. Increase subjects for above dataset, 3
5. Bound a single image with negative treatment, reward 2 cents (Threat of not paying for poor performance)
6. Bound a single image with negative treatment, increased reward to 5 cents (Threat of not paying for poor performance)
7. Increase subjects for above datasets 3 & 4 above, smaller reward. 


```{r data_summary, echo=FALSE}


format_table(
  d[, .(count=.N, mean_score=mean(bounding_box_score, na.rm=T), std_dev=sd(bounding_box_score, na.rm=T)), keyby=.(dataset_no, is_pilot, in_treatment)],
  caption = "Experiment Data Summary"
)

format_table(d[order(dataset_no), .("Mean Score"=mean(bounding_box_score, na.rm=T),
      #"95th pct"=quantile(bounding_box_score, probs=.95, na.rm=TRUE),
      "Treatment"=sum(in_treatment==1), 
      "Control"=sum(in_treatment==0),
      "Total"=sum(in_treatment %in% c(0,1)),
      "No Bouding"=sum(is.na(bounding_box_score)),
      "is_mobile"=sum(is_mobile %in% c(0,1)),
      "Reward"=max(Reward), # Each only has one
      "Std Dev"=sd(bounding_box_score, na.rm=T)
      ), 
  by=dataset_no]
)

format_table(d[order(dataset_no), .("Mean Score"=mean(bounding_box_score, na.rm=T),
      #"95th pct"=quantile(bounding_box_score, probs=.95, na.rm=TRUE),
      "Treatment"=sum(in_treatment==1), 
      "Control"=sum(in_treatment==0),
      "Total"=sum(in_treatment %in% c(0,1)),
      "No Bouding"=sum(is.na(bounding_box_score)),
      "is_mobile"=sum(is_mobile %in% c(0,1)),
      "Reward"=max(Reward), # Each only has one
      "Std Dev"=sd(bounding_box_score, na.rm=T)
      ), 
  by=experiment]
)

format_table(summary(d[, .(bounding_box_score)]))

```


## 1. Pilot

For our pilot, we gave the Turkers a social treatment suggesting their work would be used to improve a government survielance system and asked that they draw a single bounding box on each of 20 images.  We first collected some information about the subject through a survey and then randomly assigned those subjects to treatment and control.  Our primary goal was to understand how our scoring scheme worked, gauge level of variance we should expect in future experiments and test if our covariates collected from our survey explained any of the variance.  We had high attrition and due to a misunderstanding of the Mechanical Turk platform, our assignments to treatment and control failed and we ended up with Turkers not in our experiment in our results, and many ended up in both treatment and control.

We were not able to trust any ATE, but we could at least see the variance, which was exceptionally high.

### 1.1 EDA

```{r 1_plots, echo=FALSE, fig.height=5, fig.width=10, fig.show='hold'}
# Block the results by user taking their mean score

worker_mean_score <- d[experiment=="pilot", .(mean_worker_score = mean(bounding_box_score), in_treatment = as.integer(median(in_treatment))), keyby=WorkerId]


par(mfrow=c(1,2))
dev_null <- worker_mean_score[, 
  plot(mean_worker_score, col=(in_treatment+1), main="Distribution of Scores",
       xlab="MTurkers", ylab="Score")
]

dev_null <- worker_mean_score[, 
  plot(log(mean_worker_score), col=(in_treatment+1), main="Distribution of Log Scores",
       xlab="MTurkers", ylab="Log Score")
]


```


```{r 1_summary, echo=FALSE}
format_table(summary(worker_mean_score[, .(mean_worker_score)]))
format_table(worker_mean_score[, .(mean_score=mean(mean_worker_score, na.rm=T), std_dev=sd(mean_worker_score, na.rm=T)), keyby=.(in_treatment)])

```


```{r}
#TODO Gauge if effort decreases with more HITTs
```


## 2. Social Treatment

With the first pilot behind us, we decided we needed to focus on increasing our statistical power and hypothesized that collecting the same number of bounding boxes but using more subjects & fewer experiments would provide more statistical power.  Each subject was presented a single image and created a single bounding box.  


### 2.1 EDA

```{r 2_plot, echo=FALSE, fig.height=5, fig.width=10}
#dev_null <- d[dataset_no==2, plot(bounding_box_score, col=(in_treatment+1))]
#dev_null <- d[dataset_no==2, plot(log(bounding_box_score), col=(in_treatment+1))]

par(mfrow=c(1,2))
dev_null <- d[dataset_no==2, 
  plot(bounding_box_score, col=(in_treatment+1), main="Distribution of Scores",
       xlab="MTurkers", ylab="Score")
]

dev_null <- d[dataset_no==2, 
  plot(log(bounding_box_score), col=(in_treatment+1), main="Distribution of Log Scores",
       xlab="MTurkers", ylab="Log Score")
]
```
  
#### 2.1.1 Score Summary Statistics    
   
Summary Statistics for Score

```{r 2_summary, echo=FALSE}
format_table(summary(d[dataset_no==2, .(bounding_box_score)]))
```

```{r 2_summary_groups, echo=FALSE}

format_table(d[dataset_no==2, .(mean_score=mean(bounding_box_score, na.rm=T), std_dev=sd(bounding_box_score, na.rm=T)), keyby=.(in_treatment)])
```





 


### 2.2 Regression Analysis   

The results of our regression failed to show any reliable affect of our treatment.  The coeffecient is negative, which for our scoring means there is a positive influence from the treatment.  But with a p-value of 0.66 there no information can be gleaned from this with any confidence. 

```{r experiment_2_regression, results='asis', echo=FALSE}
e2_mod_1 <- d[dataset_no==2, lm(bounding_box_score ~ in_treatment)]

stargazer(e2_mod_1, 
          type=stargazer_type, header = FALSE, table.placement = 'h', report=('vc*p')
          )

```


With experiment, the only covariate we had was the amount of time each Turker spent on the task. And working time doesn't seem to be affected by our treatment. 

```{r, echo=FALSE}


#dev_null <- d[dataset_no==2, 
#  plot(x=bounding_box_score, y=WorkTimeInSeconds, col=(in_treatment+1), main="Distribution of Log Scores",
#       xlab="score", ylab="time", ylim=c(0, 200), xlim=c(0,20))
#]

```


```{r experiment_2_regression_time, echo=FALSE, results='asis'}
e2_mod_3 <- d[dataset_no==2, lm(WorkTimeInSeconds ~ in_treatment)]

stargazer(e2_mod_3,  
          type=stargazer_type, header = FALSE, table.placement = 'h', report=('vc*p'))
```

The results suggest the negative treatment caused Turkers to spend less time on the task, but the p-value is far from statistically significant again.



```{r experiment_2_regression_test, results='asis', echo=FALSE}

#e2_mod_4 <- d[dataset_no==2, lm(bounding_box_score ~ in_treatment)]
#e2_mod_5 <- d[dataset_no==2, lm(bounding_box_score ~ in_treatment)]

#stargazer(e2_mod_4,  e2_mod_5,
#          type=stargazer_type, header = FALSE, table.placement = 'h', report=('vc*p'),
#         column.labels=c("Target Alone", "With WorkInSeconds Control"))

```


### 2.3 Power Test   

To achieve the statistical power of 0.8 at the 0.05 confidence-level with the variance we had in this experiement, we would require nearly 5,800 subjects in both control and treatment. 

```{r experiment_2_power_test, echo=FALSE}

e2_ate = d[dataset_no==2 & in_treatment == 1, mean(bounding_box_score, na.rm=T)] - d[dataset_no==2 & in_treatment == 0, mean(bounding_box_score, na.rm=T)]
e2_sd = d[dataset_no==2, sd(bounding_box_score, na.rm=T)]


power_2_1 = power.t.test(delta=e2_ate, 
             sd=e2_sd, 
             sig.level = 0.05,
             power = 0.80,
             alternative = "one.sided",
             n = NULL)

print(power_2_1)
```

### 2.4 Learnings from our second experiment   

The estimated 11,600 subjects required to achieve the statistical power we needed was far too many, 
With a p-value of 0.389, even with the 11,600 subjects, we weren't likely to find a statistically significant ATE.  We need to change our experiment and collect more covariates. 



## 3 Future Payoff  

In both of our pilots, we used a treatment which we hypothesized would cause the Turkers in treatment to work less hard, and the ATE was positive, which in our scoring means the bounding was less accurate.  We also wanted to test if a positive treatment would have a larger ATE, so the Turkers in treatment were told we were looking for Turkers to perform some future work with the hypothesis that if the Turkers though of the task as a test with the incentive of future work they would try harder.  So we ran a small experiment to test this theory.

### 3.1 Initial Experiment

#### 3.1.1 EDA





#### 3.1.2 Regression Analysis
  

At first look there doesn't seem to be any significant treatment affect, the last p-value had gone down from 0.66 in the previous experiment to 0.56 in this, but we only used a quarter the number of subjects.  

```{r 3_1_simple_regression, echo=FALSE, results='asis'}
e3_mod_1 <- d[dataset_no==3, lm(bounding_box_score ~ in_treatment)]

stargazer(e3_mod_1, 
          type=stargazer_type, header = FALSE, table.placement = 'h', report=('vc*p'),
          column.labels=c("Future Payoff"))
```


#### 3.1.3 Covariate Regression Analysis

In this experiment we asked the Turkers to answer some questions about the device they were using, their experience doing these types of tasks and some demographic info. 

```{r 3_2_covariates, echo=FALSE, results='asis'}
e3_mod_2 <- d[dataset_no==3, lm(bounding_box_score ~ in_treatment+monitor)]
e3_mod_3 <- d[dataset_no==3, lm(bounding_box_score ~ in_treatment+didbf)]
e3_mod_4 <- d[dataset_no==3, lm(bounding_box_score ~ in_treatment+age)]
e3_mod_5 <- d[dataset_no==3, lm(bounding_box_score ~ in_treatment+edu)]
e3_mod_6 <- d[dataset_no==3, lm(bounding_box_score ~ in_treatment+income)]

stargazer(e3_mod_2, e3_mod_3, e3_mod_4,  
          type=stargazer_type, header = FALSE, table.placement = 'h', report=('vc*p'),
          column.labels=c("Target Alone", "Monitor size", "Did task before", "Age"),
          title="3.1.3 1")



stargazer(e3_mod_5, e3_mod_6, 
          type=stargazer_type, header = FALSE, table.placement = 'h', report=('vc*p'),
          column.labels=c("Education", "Income"),
          title="3.1.3 2")
```
  
  
The only covariate which seemed to act as any type of control was the education question, though it wasn't very significant.  However, all of the coeffecients for the screensize question were negative, and by a fairly significant ammount.  The baseline value was cellphone, which can be significantly smaller than all the other types of screens.  So we tested that on its own.
   
   
```{r experiment_3_regression_cell, echo=FALSE, results='asis'}
e3_mod_7 <- d[dataset_no==3, lm(bounding_box_score ~ in_treatment+is_mobile)]

stargazer(e3_mod_1, e3_mod_7, 
          type=stargazer_type, header = FALSE, table.placement = 'h', report=('vc*p'),
          column.labels=c("Target Alone", "Mobile Control"), title="3.1.3 2")
```


If the subject is using a cellphone to do the task, their accuracy goes down (score increases), which is intuitive.  Having cellphone as a control decreases the p-value from 0.56 to 0.077.  With more data, this could be even lower. 


As with the previous experiment, we also analyzed how the treatment affected the amount of time they spent on the task.  


```{r experiment_3_regression_time, echo=FALSE, results='asis'}
e3_mod_3 <- d[dataset_no==3, lm(WorkTimeInSeconds ~ in_treatment)]

stargazer(e3_mod_3, e2_mod_3,
          type=stargazer_type, header = FALSE, table.placement = 'h', report=('vc*p'),
          column.labels=c("Future Payoff", "Social"), title="3.1.3 2")
```

The regression shows those in our future payoff treatment on average spent 23 seconds more time.  


```{r experiment_3_summary_time, echo=FALSE, results='asis'}

hist(d[dataset_no==3, WorkTimeInSeconds], breaks = 30)

```

There are alot of values well over the reasonable it should take to perform this task, suggesting that Turkers are not conentrating on our task, it could be they are spawning multiple tabs.  Regardless, working time is not helpful for our experiment. 


#### 3.1.4 Power Test

How much more data would we need? 


```{r power_test_3, echo=FALSE}

e3_ate = d[dataset_no==3 & in_treatment == 1, mean(bounding_box_score, na.rm=T)] - d[dataset_no==3 & in_treatment == 0, mean(bounding_box_score, na.rm=T)]
e3_sd = d[dataset_no==3, sd(bounding_box_score, na.rm=T)]

power.t.test(delta=e3_ate, 
             sd=e3_sd, 
             sig.level = 0.05,
             power = 0.80,
             alternative = "one.sided",
             n = NULL)


power_curve <- function(x) {
  result = c()

  for (i in 1:length(x)) {
    new_n <- power.t.test(delta=abs(e3_ate), 
             sd=e3_sd, 
             sig.level = 0.05,
             power = NULL,
             alternative = "one.sided",
             n = x[i])["power"]
    
    result <- c(result, new_n)
  }
  
  return(result)
}

curve(power_curve(x), 10, 2000, ylab = "Statistical Power", xlab="n", main="Subjects Required for 80% Power, p-value=0.05")


```

The power calculation when using the negative treatment, telling those in treatment that they were doing work for a government surveillance system estimated we needed 5,800 subjects.  Using an incentive of possible future work as the treatment, the ATE has less variance, and estimated that we only need 835 subjects to get 0.80 statistical power. 


### 3.2 Future Payoff - Statistical Power (need better description)

To improve the statistical power for this experiment, we collected data from 600 more subjects.

#### 3.2.1


#### 3.2.2 Regression Analysis  




```{r experiment_4_regression, results='asis', echo=FALSE}
e4_mod_1 <- d[experiment == "future", lm(bounding_box_score ~ in_treatment)]

stargazer(e4_mod_1, e3_mod_1,  
          type=stargazer_type, header = TRUE, table.placement = 'h', report=('vc*p'),
          column.labels=c("n=700", "n=100"), title="3.2.2 Regression")
```


The results are much better, adding another 700 subjects helped decrease the p-value from 0.56 to 0.05, and our ATE is -11.8, a negative number means the bounding boxes from treatment are more accurate.  Controlling using mobile devices as a control, we see a much of the variance is explained by the use of mobile devices, though our p-value decreased when we used this control.   

```{r experiment_4_compare_previous, echo=FALSE, results='asis'}


e4_mod_2 <- d[experiment == "future", lm(bounding_box_score ~ in_treatment+is_mobile)]
e4_mod_3 <- d[experiment == "future", lm(bounding_box_score ~ in_treatment+Reward)]
e4_mod_4 <- d[experiment == "future", lm(bounding_box_score ~ in_treatment+is_mobile+Reward)]

stargazer(e4_mod_1, e4_mod_2, e4_mod_3, e4_mod_4,  
          type=stargazer_type, header = FALSE, table.placement = 'h', report=('vc*p'),
          column.labels=c("Target Alone", "Controlling for Mobile", "Reward", "Mobile and Reward"), title="3.2.2 2")


```




#### 3.2.3 Handling Outliers


```{r 3_2_3_EDA, echo=FALSE, fig.height=5, fig.width=10}

par(mfrow=c(1,2))
hist(d[experiment=="future" & pct_5==1, bounding_box_score], 30, xlab="score", main="Distribution 5th Percentile")
hist(d[experiment=="future", bounding_box_score], 30, xlab="score", main="Distribution 5th Percentile")

```



```{r 3_2_3_analysis, echo=FALSE, results='asis'}
#e4_mod_5 <- d[experiment == "future", lm(in_treatment ~ bounding_box_score)]
#e4_mod_6 <- d[experiment == "future", lm(in_treatment ~ pct_95)]
e4_mod_5 <- d[experiment == "future", lm(bounding_box_score ~ in_treatment+pct_95)]
e4_mod_6 <- d[experiment == "future", lm(bounding_box_score ~ in_treatment+is_mobile+pct_95)]
e4_mod_7 <- d[experiment == "future", lm(bounding_box_score ~ in_treatment+is_mobile+Reward+pct_95)]

stargazer(e4_mod_5, e4_mod_6, e4_mod_7,  
          type=stargazer_type, header = FALSE, table.placement = 'h', report=('vc*p'),
          column.labels=c("Controlling for Mobile", "Controlling for Mobile and Effort"), title="3.2.3")


```

#### 3.2.4 Covariate Balance Check

```{r 3_2_4_covariate_balance, echo=FALSE}
d[experiment == "future", .(cell=sum(is_cellphone, na.rm=T), not_cell=sum(is_cellphone==0, na.rm=T), 
                            five_cents=sum(Reward=="$0.05"), twenty_cents=sum(Reward=="$0.20")), by=in_treatment]
```

#### 3.2.5 Attrition
 
   

## 4 Immediate Payment

### 4.1 EDA

### 4.2 Regression Analysis


```{r, echo=FALSE, results='asis'}

e5_mod_1 <- d[experiment == "immediate", lm(bounding_box_score ~ in_treatment)]
summary(e5_mod_1)
```

#### 4.3 Power Test


```{r experiment_4_power_test, echo=FALSE}

e4_ate = d[experiment == "immediate" & in_treatment == 1, mean(bounding_box_score, na.rm=T)] - d[experiment == "immediate" & in_treatment == 0, mean(bounding_box_score, na.rm=T)]
e4_sd = d[experiment == "immediate", sd(bounding_box_score, na.rm=T)]


power_4 = power.t.test(delta=abs(e4_ate), 
             sd=e4_sd, 
             sig.level = 0.05,
             power = 0.80,
             alternative = "one.sided",
             n = NULL)

print(power_4)
```

## 5 Cross Comparison

```{r 5_cross, echo=FALSE, results='asis'}
e6_mod_2 <- d[dataset_no %in% c(5,6), lm(bounding_box_score ~ factor(Reward)+in_treatment)]
e6_mod_3 <- d[dataset_no %in% c(3,4,5,6), lm(bounding_box_score ~ factor(Reward)+in_treatment)]
e6_mod_4 <- d[dataset_no %in% c(3,4,5,6), lm(bounding_box_score ~ (Reward == "$0.20")+in_treatment)]


stargazer(e6_mod_2, e6_mod_3, e6_mod_4,
          type=stargazer_type, header = FALSE, table.placement = 'h', report=('vc*p'))
```




